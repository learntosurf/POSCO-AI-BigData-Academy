{"cells":[{"cell_type":"markdown","id":"0ec2fb5a","metadata":{"id":"0ec2fb5a"},"source":["# Diffusion models"]},{"cell_type":"markdown","id":"63bafc8d","metadata":{"id":"63bafc8d"},"source":["Blog posts/reviews:\n","- [Lilian Weng's blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n","- [Yang Song's blog post](https://yang-song.net/blog/2021/score/)\n","- [Alex Alemi's blog post](https://blog.alexalemi.com/diffusion.html)\n","- [Understanding Diffusion Models: A Unified Perspective by Calvin Luo](https://arxiv.org/abs/2208.11970)\n","  \n","Relevant papers:\n","- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n","- [Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456)\n","- [Maximum Likelihood Training of Score-Based Diffusion Models](https://arxiv.org/abs/2101.09258)\n","- [Variational Diffusion Models](https://arxiv.org/abs/2107.00630)\n"]},{"cell_type":"markdown","id":"6826124a","metadata":{"id":"6826124a"},"source":["## Denoising diffusion"]},{"cell_type":"markdown","id":"aca3293c","metadata":{"id":"aca3293c"},"source":["Denoising diffusion can be viewed as a hierarchical VAE, with a few restrictions on the architecture:\n","1. The dimensionality of the latents at different hierarchical levels and the data are the same (although this can be relaxed for latent diffusion).\n","2. The encoder is not learned; instead, the encoding processes are Markovian $q(x_t\\mid x_{t-1})$ and correspond to a Gaussian distributions with mean corresponding to the output of the previous encoding step, $$q\\left({x}_t \\mid {x}_{t-1}\\right)=\\mathcal{N}\\left(\\sqrt{1 - \\beta_t}\\, {x}_{t-1},\\beta_t \\mathbb{I}\\right)$$\n","where $\\beta_t$ is a fixed or learnable variance schedule, and we've decided that the variance of the latents is preserved at various hierarchies (timesteps). This is the forward diffusion process.\n","3. The distribution of latents at the final timestep is a standard Gaussian (similar to a regular VAE).\n","\n","### The forward process\n","\n","We have $q\\left({x}_t \\mid {x}_{t-1}\\right)=\\mathcal{N}\\left(\\sqrt{1 - \\beta_t}\\, {x}_{t-1},\\beta_t \\mathbb{I}\\right)$. During training, it will be useful to have a closed-form expression for $q(x_t\\mid x_0)$. See Eqs. (61)-(70) of [2208.11970](https://arxiv.org/abs/2208.11970); with $\\alpha_t\\equiv 1 - \\beta_t$ we have\n","$$q(x_t\\mid x_0) = \\mathcal N(\\sqrt{(\\overline{\\alpha}_t)} x_0, \\sqrt{(1 - \\overline{\\alpha}_t)}\\mathbb I )$$\n","with $\\overline{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$ which is sometimes called the diffusion kernel.\n","\n","### Optimizing variational lower bound\n","\n","As for any VAE and exactly as in the previous notebook, we can optimize a variational lower bound on the log-evidence through the ELBO by enforcing a consistency between forward and reverse diffusion processes at each timestep. This gives\n","$$\\mathrm{ELBO} = L_0 + \\sum_{t=2}^T L_t + L_T$$\n","where $L_0 = \\log p_\\theta(x_0\\mid x_1)$ is a reconstruction loss, with data samples $x_0 \\mid p(x_0)$. $L_T = -D_\\mathrm{KL}\\left(q_\\phi(x_T|x_0)\\,\\|\\,p(x_T)\\right)$ is the prior matching term with $p(x_T) = \\mathcal N(0, \\mathbb I)$, and\n","$$L_t = -D_{\\mathrm{KL}}\\left(q\\left({x}_{t - 1} \\mid {x}_{t}, {x}_0\\right) \\|\\,p_\\theta\\left({x}_{t-1} \\mid {x}_{t}\\right)\\right)$$\n","is the diffusion consistency term which imposes a consistency between the forward and reverse diffusion processes. $q(x_{t-1}\\mid x_t)$ has an analytic form, with variance\n","$$\n","\\sigma_q^2(t)=\\frac{\\left(1-\\alpha_t\\right)\\left(1-\\overline{\\alpha}_{t-1}\\right)}{1-\\overline{\\alpha}_t}.\n","$$\n","The variances of the forward and reverse processes can be enforced to be the same, so computing the KL-divergence essentially boils down to matching the means. Eventually, the diffusion consistency loss simplifies as\n","$$\n","\\frac{1}{2 \\sigma_q^2(t)} \\frac{\\overline{\\alpha}_{t-1}\\beta_t^2}{\\left(1-\\overline{\\alpha}_t\\right)^2}\\left[\\left\\|\\hat{{x}}_{{\\theta}}\\left({x}_t, t\\right)-{x}_0\\right\\|^2\\right]\n","$$\n","and equivalently as\n","$$\n","\\frac{1}{2 \\sigma_q^2(t)} \\frac{\\beta_t^2}{\\left(1-\\overline{\\alpha}_t\\right) \\alpha_t}\\left[\\left\\|\\hat{{\\epsilon}}_{{\\theta}}\\left({x}_t, t\\right) - {\\epsilon}_0\\right\\|^2\\right]\n","$$\n","which correspond to original-data prediction and noise prediction, respectively."]},{"cell_type":"markdown","id":"70e4b826","metadata":{"id":"70e4b826"},"source":["### Implementation"]},{"cell_type":"markdown","id":"84443300","metadata":{"id":"84443300"},"source":["* Model: 노이즈 예측 모델 (UNet, MLP 등).\n","* Scheduler: Noise 스케줄링 및 샘플링 방법 관리.\n","* Trainer: 모델 학습 및 샘플링 과정을 관리."]},{"cell_type":"markdown","id":"0e1b7240","metadata":{"id":"0e1b7240"},"source":["#### 1. Model"]},{"cell_type":"code","execution_count":1,"id":"0d8867e9","metadata":{"id":"0d8867e9","executionInfo":{"status":"ok","timestamp":1725794278247,"user_tz":-540,"elapsed":8648,"user":{"displayName":"허성우","userId":"11849707056234337549"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","\n","class GaussianFourierProjection(nn.Module):\n","  \"\"\"Gaussian random features for encoding time steps.\"\"\"\n","  def __init__(self, embed_dim, scale=30.):\n","    super().__init__()\n","    # Randomly sample weights (frequencies) during initialization.\n","    # These weights (frequencies) are fixed during optimization and are not trainable.\n","    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n","  def forward(self, x):\n","    # Cosine(2 pi freq x), Sine(2 pi freq x)\n","    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n","    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n","\n","\n","class Dense(nn.Module):\n","  \"\"\"A fully connected layer that reshapes outputs to feature maps.\n","  Allow time repr to input additively from the side of a convolution layer.\n","  \"\"\"\n","  def __init__(self, input_dim, output_dim):\n","    super().__init__()\n","    self.dense = nn.Linear(input_dim, output_dim)\n","  def forward(self, x):\n","    # this broadcast the 2d tensor to 4d, add the same value across space.\n","    return self.dense(x)[..., None, None]\n","\n","\n","class UNet(nn.Module):\n","  \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n","\n","  def __init__(self, channels=[32, 64, 128, 256], embed_dim=256):\n","    \"\"\"Initialize a time-dependent score-based network.\n","\n","    Args:\n","      channels: The number of channels for feature maps of each resolution.\n","      embed_dim: The dimensionality of Gaussian random feature embeddings.\n","    \"\"\"\n","    super().__init__()\n","    # Gaussian random feature embedding layer for time\n","    self.time_embed = nn.Sequential(\n","          GaussianFourierProjection(embed_dim=embed_dim),\n","          nn.Linear(embed_dim, embed_dim)\n","          )\n","    # Encoding layers where the resolution decreases\n","    self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n","    self.t_mod1 = Dense(embed_dim, channels[0])\n","    self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n","\n","    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n","    self.t_mod2 = Dense(embed_dim, channels[1])\n","    self.gnorm2 = nn.GroupNorm(8, num_channels=channels[1])\n","\n","    self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n","    self.t_mod3 = Dense(embed_dim, channels[2])\n","    self.gnorm3 = nn.GroupNorm(8, num_channels=channels[2])\n","\n","    self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n","    self.t_mod4 = Dense(embed_dim, channels[3])\n","    self.gnorm4 = nn.GroupNorm(8, num_channels=channels[3])\n","\n","\n","    # Decoding layers where the resolution increases\n","    self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n","    self.t_mod5 = Dense(embed_dim, channels[2])\n","    self.tgnorm4 = nn.GroupNorm(8, num_channels=channels[2])\n","    self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)\n","    self.t_mod6 = Dense(embed_dim, channels[1])\n","    self.tgnorm3 = nn.GroupNorm(8, num_channels=channels[1])\n","    self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)\n","    self.t_mod7 = Dense(embed_dim, channels[0])\n","    self.tgnorm2 = nn.GroupNorm(8, num_channels=channels[0])\n","    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n","\n","    # The swish activation function\n","    self.act = lambda x: x * torch.sigmoid(x)\n","    # A restricted version of the `marginal_prob_std` function, after specifying a Lambda.\n","    # self.marginal_prob_std = marginal_prob_std\n","\n","  def forward(self, x, t, y=None):\n","    # Obtain the Gaussian random feature embedding for t\n","    embed = self.act(self.time_embed(t))\n","    # Encoding path, downsampling\n","    ## Incorporate information from t\n","    h1 = self.conv1(x)  + self.t_mod1(embed)\n","    ## Group normalization  and  apply activation function\n","    h1 = self.act(self.gnorm1(h1))\n","    #  2nd conv\n","    h2 = self.conv2(h1) + self.t_mod2(embed)\n","    h2 = self.act(self.gnorm2(h2))\n","    # 3rd conv\n","    h3 = self.conv3(h2) + self.t_mod3(embed)\n","    h3 = self.act(self.gnorm3(h3))\n","    # 4th conv\n","    h4 = self.conv4(h3) + self.t_mod4(embed)\n","    h4 = self.act(self.gnorm4(h4))\n","\n","    # Decoding path up sampling\n","    h = self.tconv4(h4) + self.t_mod5(embed)\n","    ## Skip connection from the encoding path\n","    h = self.act(self.tgnorm4(h))\n","    h = self.tconv3(torch.cat([h, h3], dim=1)) + self.t_mod6(embed)\n","    h = self.act(self.tgnorm3(h))\n","    h = self.tconv2(torch.cat([h, h2], dim=1)) + self.t_mod7(embed)\n","    h = self.act(self.tgnorm2(h))\n","    h = self.tconv1(torch.cat([h, h1], dim=1))\n","\n","    # Normalize output\n","    # h = h / self.marginal_prob_std(t)[:, None, None, None]\n","    return h"]},{"cell_type":"code","execution_count":2,"id":"195996a3","metadata":{"id":"195996a3","executionInfo":{"status":"ok","timestamp":1725794278247,"user_tz":-540,"elapsed":3,"user":{"displayName":"허성우","userId":"11849707056234337549"}}},"outputs":[],"source":["# t_emb = model.time_embedding(torch.tensor([0])).unsqueeze(-1).unsqueeze(-1)"]},{"cell_type":"markdown","id":"477888ad","metadata":{"id":"477888ad"},"source":["#### 2. Scheduler"]},{"cell_type":"code","execution_count":9,"id":"27a928c3","metadata":{"id":"27a928c3","executionInfo":{"status":"ok","timestamp":1725794481760,"user_tz":-540,"elapsed":287,"user":{"displayName":"허성우","userId":"11849707056234337549"}}},"outputs":[],"source":["import numpy as np\n","\n","class DDPMScheduler:\n","    def __init__(self, T, beta_start=1e-4, beta_end=0.02):\n","        self.T = T\n","        self.betas = np.linspace(beta_start, beta_end, T)\n","        self.alphas = 1 - self.betas\n","        self.alpha_bars = np.cumprod(self.alphas)\n","\n","    def get_alpha_bar(self, t):\n","        return self.alpha_bars[t]\n","\n","    def get_alpha(self, t):\n","        return self.alphas[t]\n","\n","    def add_noise(self, x_0, t):\n","        \"\"\"\n","        Forward diffusion: x_0에 노이즈를 추가한 샘플 생성\n","        \"\"\"\n","        noise = torch.randn_like(x_0)\n","        sqrt_alpha_bar = torch.sqrt(torch.tensor(self.alpha_bars[t], dtype=torch.float32, device= x_0.device))\n","        sqrt_one_minus_alpha_bar = torch.sqrt(1 - torch.tensor(self.alpha_bars[t], dtype=torch.float32, device= x_0.device))\n","        return sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * noise, noise"]},{"cell_type":"markdown","id":"891898e5","metadata":{"id":"891898e5"},"source":["#### 3. Trainer"]},{"cell_type":"code","execution_count":10,"id":"977caac1","metadata":{"id":"977caac1","executionInfo":{"status":"ok","timestamp":1725794485870,"user_tz":-540,"elapsed":295,"user":{"displayName":"허성우","userId":"11849707056234337549"}}},"outputs":[],"source":["import os\n","from glob import glob\n","\n","class DDPMTrainer:\n","    def __init__(self, model, scheduler, lr=1e-3):\n","        self.model = model\n","        self.scheduler = scheduler\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n","        self.loss_fn = nn.MSELoss()\n","\n","    def train_step(self, x_0, t):\n","        x_noisy, noise = self.scheduler.add_noise(x_0, t)\n","        predicted_noise = self.model(x_noisy, torch.tensor([t], device= x_noisy.device))\n","        loss = self.loss_fn(predicted_noise, noise)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        return loss.item()\n","\n","    @torch.no_grad()\n","    def sample(self, x_shape):\n","        \"\"\"\n","        DDPM 샘플링 과정: 노이즈에서 원본 이미지를 복원\n","        \"\"\"\n","        device = next(self.model.parameters()).device\n","        x = torch.randn(x_shape).to(device)\n","        for t in reversed(range(self.scheduler.T)):\n","            alpha_bar_t = torch.tensor(self.scheduler.get_alpha_bar(t), dtype=torch.float32, device=device)\n","            predicted_noise = self.model(x, torch.tensor([t], device=device))\n","            x = (x - (1 - alpha_bar_t).sqrt() * predicted_noise) / alpha_bar_t.sqrt()\n","            if t > 0:\n","                noise = torch.randn_like(x)\n","                x += (1 - alpha_bar_t).sqrt() * noise\n","        return x\n","\n","\n","    def save_ckpt(self, save_dir, epoch=None):\n","        try:\n","            # 디렉토리 생성\n","            os.makedirs(save_dir, exist_ok=True)\n","\n","            # epoch가 None인 경우 기본값 설정\n","            if epoch is None:\n","                epoch = 1\n","\n","            # 체크포인트 파일 경로\n","            ckpt_file = os.path.join(save_dir, f'ckpt_epoch_{epoch}.pt')\n","\n","            # 체크포인트 저장\n","            torch.save(\n","                {\n","                    'model': self.model.state_dict(),\n","                    'optimizer': self.optimizer.state_dict(),\n","                    'epoch': epoch\n","                }, ckpt_file\n","            )\n","\n","            print(f\"Checkpoint saved to {ckpt_file}\")\n","\n","        except Exception as e:\n","            print(f\"Error saving checkpoint: {e}\")\n","            raise IOError()\n"]},{"cell_type":"markdown","id":"c84ade58","metadata":{"id":"c84ade58"},"source":["### Training"]},{"cell_type":"markdown","id":"87cd7d6d","metadata":{"id":"87cd7d6d"},"source":["##### Data\n","\n","* MNIST 사용\n","<p align=\"center\">\n"," <img src = \"./hugging_face/figs/mnist.png\", height=\"256\">\n","</p>\n"]},{"cell_type":"code","execution_count":16,"id":"9a3bf7d4","metadata":{"id":"9a3bf7d4","executionInfo":{"status":"ok","timestamp":1725794712030,"user_tz":-540,"elapsed":279,"user":{"displayName":"허성우","userId":"11849707056234337549"}}},"outputs":[],"source":["import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# MNIST 데이터셋 로드 및 전처리\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # 데이터를 텐서로 변환\n","])\n","\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n"]},{"cell_type":"markdown","id":"d597ae6a","metadata":{"id":"d597ae6a"},"source":["#### Training"]},{"cell_type":"code","execution_count":17,"id":"aaee14b2","metadata":{"id":"aaee14b2","executionInfo":{"status":"ok","timestamp":1725794718723,"user_tz":-540,"elapsed":279,"user":{"displayName":"허성우","userId":"11849707056234337549"}}},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":18,"id":"41588637","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41588637","executionInfo":{"status":"ok","timestamp":1725794863733,"user_tz":-540,"elapsed":144727,"user":{"displayName":"허성우","userId":"11849707056234337549"}},"outputId":"aa1aae6b-d14f-4e87-9720-4fbf92ba6011"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Batch: 0, Time step: 629, Loss: 2.637397527694702\n","Epoch: 0, Batch: 100, Time step: 447, Loss: 1.4568110704421997\n","Epoch: 0, Batch: 200, Time step: 536, Loss: 1.003360629081726\n","Epoch: 1, Batch: 0, Time step: 441, Loss: 0.8924155831336975\n","Epoch: 1, Batch: 100, Time step: 966, Loss: 0.6910971999168396\n","Epoch: 1, Batch: 200, Time step: 716, Loss: 0.613950252532959\n","Training complete!\n"]}],"source":["# Hyperparameters\n","T = 1000  # Diffution time steps\n","channels=[8, 16, 32, 64]\n","embed_dim=64\n","\n","# Model, scheduler, and trainer initialization\n","# model = SimpleCNN().to(device)\n","model = UNet(channels=channels, embed_dim= embed_dim).to(device)\n","scheduler = DDPMScheduler(T=T)\n","trainer = DDPMTrainer(model=model, scheduler=scheduler, lr=1e-3)\n","\n","# Training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    for batch_idx, (x_0, _) in enumerate(train_loader):\n","        x_0 = x_0.to(device)\n","        t = torch.randint(0, T, (1,)).item()  # random time stamp from Uniform distribution.\n","        loss = trainer.train_step(x_0, t)\n","        if batch_idx % 100 == 0:\n","            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Time step: {t}, Loss: {loss}\")\n","\n","print(\"Training complete!\")"]},{"cell_type":"markdown","id":"f45bab50","metadata":{"id":"f45bab50"},"source":["### Sampling\n","\n","The simple way to sample is through ancestral sampling from\n","$$q(x_0\\mid x_T) = \\prod_{i = 0}^{t-1} q(x_t \\mid x_{t + 1})$$\n","with\n","$$q(x_{t - 1} \\mid x_{t}) = \\mathcal N\\left(\\frac{1}{\\sqrt{\\alpha_t}}\\left({x}_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}} {\\epsilon}_\\theta\\left({x}_t, t\\right)\\right), \\sigma_t \\mathbb I\\right)\n","$$\n","where $x_T \\sim \\mathcal N(0, \\mathbb I)$."]},{"cell_type":"code","execution_count":null,"id":"a2cc4fa5","metadata":{"id":"a2cc4fa5","executionInfo":{"status":"aborted","timestamp":1725794686818,"user_tz":-540,"elapsed":2,"user":{"displayName":"허성우","userId":"11849707056234337549"}}},"outputs":[],"source":["# !pip install matplotlib"]},{"cell_type":"code","execution_count":20,"id":"b9cffc93","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136},"id":"b9cffc93","executionInfo":{"status":"ok","timestamp":1725794886085,"user_tz":-540,"elapsed":7494,"user":{"displayName":"허성우","userId":"11849707056234337549"}},"outputId":"7a0bca18-7f72-4e54-b31b-0b5e9dcd8d67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sampling from model...\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x100 with 8 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","\n","def plot_images(images, title=\"Generated Images\"):\n","    # Visualize the sampled images\n","    images = images.view(-1, 28, 28).detach().cpu().numpy()\n","    fig, axes = plt.subplots(1, len(images), figsize=(len(images), 1))\n","    for i, ax in enumerate(axes):\n","        ax.imshow(images[i], cmap='gray')\n","        ax.axis('off')\n","    plt.suptitle(title)\n","    plt.show()\n","\n","# Sampling\n","print(\"Sampling from model...\")\n","sampled_images = trainer.sample(x_shape=(8, 1, 28, 28))  # 8 개의 이미지를 샘플링\n","plot_images(sampled_images, title=\"Generated MNIST Images\")"]},{"cell_type":"code","source":[],"metadata":{"id":"sjHJjbGrYUb3"},"id":"sjHJjbGrYUb3","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}